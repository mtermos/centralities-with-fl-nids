{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from logging import INFO, DEBUG\n",
    "from flwr.common.logger import log\n",
    "\n",
    "\n",
    "from src.models.evaluation_metrics import custom_acc_mc, custom_acc_binary\n",
    "\n",
    "from src.data.dataset_info import datasets\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf/\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# choosing the dataset\n",
    "dataset = datasets[0]\n",
    "print(\"dataset: {}\".format(dataset.name))\n",
    "folder_path = \"path to dataset folder\"\n",
    "\n",
    "learning_rate = 0.001\n",
    "LAMBD_1 = 0.0001\n",
    "LAMBD_2 = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_paths = [\n",
    "    folder_path + \"client_0.parquet\",\n",
    "    folder_path + \"client_1.parquet\",\n",
    "    folder_path + \"client_2.parquet\",\n",
    "    folder_path + \"client_3.parquet\",\n",
    "    folder_path + \"client_4.parquet\",\n",
    "    folder_path + \"client_5.parquet\",\n",
    "    folder_path + \"client_6.parquet\",\n",
    "    folder_path + \"client_7.parquet\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# the input dimension of the training set\n",
    "# input_dim = df.shape[1] - len(drop_columns) - len(weak_columns) - 1  # for the label_column\n",
    "  \n",
    "# specifying the number of classes, since it is different from one dataset to another and also if binary or multi-class classification\n",
    "classes_set = {\"benign\", \"attack\"}\n",
    "labels_names = {0: \"benign\", 1: \"attack\"}\n",
    "num_classes = 2\n",
    "if cfg.multi_class:\n",
    "    with open(folder_path + \"labels_names.pkl\", 'rb') as f:\n",
    "        labels_names, classes_set = pickle.load(f)\n",
    "    num_classes = len(classes_set)\n",
    "    \n",
    "labels_names = {int(k): v for k, v in labels_names.items()}\n",
    "\n",
    "print(f\"==>> classes_set: {classes_set}\")\n",
    "print(f\"==>> num_classes: {num_classes}\")\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "if cfg.multi_class:\n",
    "    test[dataset.label_col] = test[dataset.class_num_col]\n",
    "    \n",
    "test.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "test.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    test_by_class = {}\n",
    "    classes = test[dataset.class_col].unique()\n",
    "    for class_value in classes:\n",
    "        test_class = test[test[dataset.class_col] == class_value].copy()\n",
    "        test_class.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "        test_class.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "        test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_class_labels = test_class[dataset.label_col].to_numpy()\n",
    "        test_class = test_class.drop([dataset.label_col], axis=1).to_numpy()\n",
    "\n",
    "        test_by_class[class_value] = (test_class, test_class_labels)\n",
    "    \n",
    "    \n",
    "test.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "test.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "test_labels = test[dataset.label_col].to_numpy()\n",
    "test = test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "input_dim = test.shape[1]\n",
    "\n",
    "client_data = []\n",
    "for client_path in clients_paths:\n",
    "    client_data.append(pd.read_parquet(client_path))\n",
    "    \n",
    "for i in range(len(client_data)):\n",
    "    \n",
    "    cdata = client_data[i]\n",
    "    \n",
    "    if cfg.multi_class:\n",
    "        cdata[dataset.label_col] = cdata[dataset.class_num_col]\n",
    "        \n",
    "    cdata.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "    cdata.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "\n",
    "    cdata.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "    cdata.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "    cdata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "    y_train = c_train[dataset.label_col].to_numpy()\n",
    "    x_train = c_train.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    y_test = c_test[dataset.label_col].to_numpy()\n",
    "    x_test = c_test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    \n",
    "    client_data[i] = (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, Input, regularizers, callbacks, metrics, optimizers, initializers\n",
    "# from src.models.evaluation_metrics import f1_m\n",
    "\n",
    "def create_keras_model(input_shape, alpha = learning_rate):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv1D(80, kernel_size=5,\n",
    "                activation=\"relu\", input_shape=(input_shape, 1), kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2)))\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    \n",
    "    model.add(layers.Conv1D(80, 5, activation='relu', kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2)))\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    \n",
    "    model.add(layers.LSTM(units=80,\n",
    "                            kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "                            recurrent_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "                            bias_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "                            return_sequences=False,\n",
    "                            ))\n",
    "\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    model.add(layers.Dense(500,activation='relu', kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    model.add(layers.Dense(200,activation='relu', kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    model.add(layers.Dense(80,activation='relu', kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "\n",
    "    if cfg.multi_class:\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=alpha),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=alpha),\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_keras_model(input_dim)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = {}\n",
    "results_final[\"baseline\"] = {}\n",
    "results_final[\"baseline\"][\"accuracy\"] = {}\n",
    "results_final[\"baseline\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"centralities - DiGraph\"] = {}\n",
    "results_final[\"centralities - DiGraph\"][\"accuracy\"] = {}\n",
    "results_final[\"centralities - DiGraph\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"centralities - MultiDiGraph\"] = {}\n",
    "results_final[\"centralities - MultiDiGraph\"][\"accuracy\"] = {}\n",
    "results_final[\"centralities - MultiDiGraph\"][\"f1s\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"2dt - baseline\"\n",
    "results[\"dtime\"] = dtime\n",
    "results[\"multi_class\"] = cfg.multi_class\n",
    "results[\"learning_rate\"] = learning_rate\n",
    "results[\"dataset_name\"] = dataset.name\n",
    "results[\"num_classes\"] = num_classes\n",
    "results[\"labels_names\"] = labels_names\n",
    "results[\"input_dim\"] = input_dim\n",
    "\n",
    "results[\"scores\"] = {}\n",
    "results[\"scores\"][\"server\"] = {}\n",
    "results[\"scores\"][\"clients\"] = {}\n",
    "results[\"scores\"][\"accuracy\"] = {}\n",
    "results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    results[\"scores\"][\"test_by_class\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "    for k in test_by_class.keys():\n",
    "        results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, x_train, y_train, x_test, y_test):\n",
    "        self.cid = cid\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \n",
    "        lr=float(config[\"lr\"])\n",
    "        self.model = create_keras_model(input_shape=input_dim, alpha=lr)\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/baseline/client_{}\".format(dtime, self.cid)\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                 epochs=config[\"local_epochs\"],\n",
    "                                 batch_size=config[\"batch_size\"],\n",
    "                                 validation_data=(self.x_test, self.y_test),\n",
    "                                 verbose=0,\n",
    "                                 callbacks=[tensorboard_callback])\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn():\n",
    "    def client_fn(cid: str):\n",
    "        i = int(cid)\n",
    "        return FLClient(cid, client_data[i][0], client_data[i][1], client_data[i][2], client_data[i][3]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        alpha = learning_rate\n",
    "        if server_round > 5:\n",
    "            alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"lr\": alpha,\n",
    "            \"local_epochs\": config.local_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "\n",
    "def get_evaluate_fn(x_test_sever, y_test_server):\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        # eval_model = model\n",
    "        eval_model = create_keras_model(input_shape=input_dim)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/baseline/server\".format(dtime) \n",
    "        # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "                                                  batch_size = cfg.config_fit.batch_size,\n",
    "                                                  callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "        y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "        if cfg.multi_class:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            scores = custom_acc_mc(y_test_server, y_pred)\n",
    "        else:\n",
    "            y_pred = np.transpose(y_pred)[0]\n",
    "            y_pred = list(\n",
    "                map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "            scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        results_final[\"baseline\"][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "        results_final[\"baseline\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "        if not cfg.multi_class:\n",
    "            for k in test_by_class.keys():\n",
    "                y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "                y_pred_class = np.transpose(y_pred_class)[0]\n",
    "                y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "                scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "                results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "                results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "        log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "        return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics):\n",
    "    print(f\"==>> weighted_average: {metrics}\")\n",
    "\n",
    "    total_examples = 0\n",
    "    federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "    for num_examples, m in metrics:\n",
    "        for k, v in m.items():\n",
    "            federated_metrics[k] += num_examples * v\n",
    "        total_examples += num_examples\n",
    "    return {k: v / total_examples for k, v in federated_metrics.items()}\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "    min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "    fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "    min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "    min_available_clients=len(client_data),  # total clients in the simulation\n",
    "    fit_metrics_aggregation_fn = weighted_average,\n",
    "    # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "    on_fit_config_fn=get_on_fit_config(\n",
    "        cfg.config_fit\n",
    "    ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "    evaluate_fn=get_evaluate_fn(test, test_labels),\n",
    ")  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from math import floor\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=generate_client_fn(),  # a function that spawns a particular client\n",
    "    # num_clients=cfg.n_clients,  # total number of clients\n",
    "    num_clients=len(client_data),  # total number of clients\n",
    "    config=fl.server.ServerConfig(\n",
    "        num_rounds=cfg.n_rounds\n",
    "        # num_rounds=5\n",
    "    ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "    strategy=strategy,  # our strategy of choice\n",
    "    client_resources={\n",
    "        \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "        \"num_gpus\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> history: {history}\")\n",
    "print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results/{}'.format(dtime)):\n",
    "    os.mkdir('./results/{}'.format(dtime))\n",
    "\n",
    "# if not os.path.isdir('./results/{}'.format(dataset_name)):\n",
    "#     os.mkdir('./results/{}'.format(dataset_name))\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "filename = ('./results/{}/baseline.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralities - DiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "if cfg.multi_class:\n",
    "    test[dataset.label_col] = test[dataset.class_num_col]\n",
    "    \n",
    "# test.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "test.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    test_by_class = {}\n",
    "    classes = test[dataset.class_col].unique()\n",
    "    for class_value in classes:\n",
    "        test_class = test[test[dataset.class_col] == class_value].copy()\n",
    "        test_class.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "        test_class.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "        test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_class_labels = test_class[dataset.label_col].to_numpy()\n",
    "        test_class = test_class.drop([dataset.label_col], axis=1).to_numpy()\n",
    "\n",
    "        test_by_class[class_value] = (test_class, test_class_labels)\n",
    "    \n",
    "    \n",
    "test.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "test.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "test_labels = test[dataset.label_col].to_numpy()\n",
    "test = test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "input_dim = test.shape[1]\n",
    "\n",
    "client_data = []\n",
    "for client_path in clients_paths:\n",
    "    client_data.append(pd.read_parquet(client_path))\n",
    "    \n",
    "for i in range(len(client_data)):\n",
    "    \n",
    "    cdata = client_data[i]\n",
    "    \n",
    "    if cfg.multi_class:\n",
    "        cdata[dataset.label_col] = cdata[dataset.class_num_col]\n",
    "        \n",
    "    # cdata.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "    cdata.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "\n",
    "    cdata.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "    cdata.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "    cdata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "    y_train = c_train[dataset.label_col].to_numpy()\n",
    "    x_train = c_train.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    y_test = c_test[dataset.label_col].to_numpy()\n",
    "    x_test = c_test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    \n",
    "    client_data[i] = (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"2dt - Centralities - DiGraph\"\n",
    "results[\"dtime\"] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "results[\"multi_class\"] = cfg.multi_class\n",
    "results[\"learning_rate\"] = learning_rate\n",
    "results[\"dataset_name\"] = dataset.name\n",
    "results[\"num_classes\"] = num_classes\n",
    "results[\"labels_names\"] = labels_names\n",
    "results[\"input_dim\"] = input_dim\n",
    "\n",
    "results[\"scores\"] = {}\n",
    "results[\"scores\"][\"server\"] = {}\n",
    "results[\"scores\"][\"clients\"] = {}\n",
    "results[\"scores\"][\"accuracy\"] = {}\n",
    "results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    results[\"scores\"][\"test_by_class\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "    for k in test_by_class.keys():\n",
    "        results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_keras_model(input_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, x_train, y_train, x_test, y_test):\n",
    "        self.cid = cid\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \n",
    "        lr=float(config[\"lr\"])\n",
    "        # self.model = create_keras_model(input_shape= self.x_train.shape[1], alpha=lr)\n",
    "        self.model = create_keras_model(input_shape=input_dim, alpha=lr)\n",
    "        # log(INFO, f\"==>> config: {config}\")\n",
    "        # log(INFO, f\"==>> float(config[lr]): {lr}\")\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/digraph/client_{}\".format(dtime, self.cid)\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                 epochs=config[\"local_epochs\"],\n",
    "                                 batch_size=config[\"batch_size\"],\n",
    "                                 validation_data=(self.x_test, self.y_test),\n",
    "                                 verbose=0,\n",
    "                                 callbacks=[tensorboard_callback])\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn():\n",
    "    def client_fn(cid: str):\n",
    "        i = int(cid)\n",
    "        return FLClient(cid, client_data[i][0], client_data[i][1], client_data[i][2], client_data[i][3]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        alpha = learning_rate\n",
    "        if server_round > 5:\n",
    "            alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"lr\": alpha,\n",
    "            \"local_epochs\": config.local_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "\n",
    "def get_evaluate_fn(x_test_sever, y_test_server):\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        # eval_model = model\n",
    "        eval_model = create_keras_model(input_shape=input_dim)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/digraph/server\".format(dtime) \n",
    "        # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "                                                  batch_size = cfg.config_fit.batch_size,\n",
    "                                                  callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "        y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "        if cfg.multi_class:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            scores = custom_acc_mc(y_test_server, y_pred)\n",
    "        else:\n",
    "            y_pred = np.transpose(y_pred)[0]\n",
    "            y_pred = list(\n",
    "                map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "            scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        results_final[\"centralities - DiGraph\"][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "        results_final[\"centralities - DiGraph\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "        if not cfg.multi_class:\n",
    "            for k in test_by_class.keys():\n",
    "                y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "                y_pred_class = np.transpose(y_pred_class)[0]\n",
    "                y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "                scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "                results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "                results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "        log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "        return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics):\n",
    "    print(f\"==>> weighted_average: {metrics}\")\n",
    "\n",
    "    total_examples = 0\n",
    "    federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "    for num_examples, m in metrics:\n",
    "        for k, v in m.items():\n",
    "            federated_metrics[k] += num_examples * v\n",
    "        total_examples += num_examples\n",
    "    return {k: v / total_examples for k, v in federated_metrics.items()}\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "    min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "    fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "    min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "    min_available_clients=len(client_data),  # total clients in the simulation\n",
    "    fit_metrics_aggregation_fn = weighted_average,\n",
    "    # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "    on_fit_config_fn=get_on_fit_config(\n",
    "        cfg.config_fit\n",
    "    ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "    evaluate_fn=get_evaluate_fn(test, test_labels),\n",
    ")  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from math import floor\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=generate_client_fn(),  # a function that spawns a particular client\n",
    "    # num_clients=cfg.n_clients,  # total number of clients\n",
    "    num_clients=len(client_data),  # total number of clients\n",
    "    config=fl.server.ServerConfig(\n",
    "        num_rounds=cfg.n_rounds\n",
    "        # num_rounds=5\n",
    "    ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "    strategy=strategy,  # our strategy of choice\n",
    "    client_resources={\n",
    "        \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "        \"num_gpus\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> history: {history}\")\n",
    "print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/digraph.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralities - MultiDiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "if cfg.multi_class:\n",
    "    test[dataset.label_col] = test[dataset.class_num_col]\n",
    "    \n",
    "test.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "# test.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    test_by_class = {}\n",
    "    classes = test[dataset.class_col].unique()\n",
    "    for class_value in classes:\n",
    "        test_class = test[test[dataset.class_col] == class_value].copy()\n",
    "        test_class.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "        test_class.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "        test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_class_labels = test_class[dataset.label_col].to_numpy()\n",
    "        test_class = test_class.drop([dataset.label_col], axis=1).to_numpy()\n",
    "\n",
    "        test_by_class[class_value] = (test_class, test_class_labels)\n",
    "    \n",
    "    \n",
    "test.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "test.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "test_labels = test[dataset.label_col].to_numpy()\n",
    "test = test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "input_dim = test.shape[1]\n",
    "\n",
    "client_data = []\n",
    "for client_path in clients_paths:\n",
    "    client_data.append(pd.read_parquet(client_path))\n",
    "    \n",
    "for i in range(len(client_data)):\n",
    "    \n",
    "    cdata = client_data[i]\n",
    "    \n",
    "    if cfg.multi_class:\n",
    "        cdata[dataset.label_col] = cdata[dataset.class_num_col]\n",
    "        \n",
    "    cdata.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "    # cdata.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "\n",
    "    cdata.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "    cdata.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "    cdata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "    y_train = c_train[dataset.label_col].to_numpy()\n",
    "    x_train = c_train.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    y_test = c_test[dataset.label_col].to_numpy()\n",
    "    x_test = c_test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    \n",
    "    client_data[i] = (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"2dt - Centralities - MultiDiGraph\"\n",
    "results[\"dtime\"] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "results[\"multi_class\"] = cfg.multi_class\n",
    "results[\"learning_rate\"] = learning_rate\n",
    "results[\"dataset_name\"] = dataset.name\n",
    "results[\"num_classes\"] = num_classes\n",
    "results[\"labels_names\"] = labels_names\n",
    "results[\"input_dim\"] = input_dim\n",
    "\n",
    "results[\"scores\"] = {}\n",
    "results[\"scores\"][\"server\"] = {}\n",
    "results[\"scores\"][\"clients\"] = {}\n",
    "results[\"scores\"][\"accuracy\"] = {}\n",
    "results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    results[\"scores\"][\"test_by_class\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "    for k in test_by_class.keys():\n",
    "        results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_keras_model(input_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, x_train, y_train, x_test, y_test):\n",
    "        self.cid = cid\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \n",
    "        lr=float(config[\"lr\"])\n",
    "        # self.model = create_keras_model(input_shape= self.x_train.shape[1], alpha=lr)\n",
    "        self.model = create_keras_model(input_shape=input_dim, alpha=lr)\n",
    "        # log(INFO, f\"==>> config: {config}\")\n",
    "        # log(INFO, f\"==>> float(config[lr]): {lr}\")\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/multidigraph/client_{}\".format(dtime, self.cid)\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                 epochs=config[\"local_epochs\"],\n",
    "                                 batch_size=config[\"batch_size\"],\n",
    "                                 validation_data=(self.x_test, self.y_test),\n",
    "                                 verbose=0,\n",
    "                                 callbacks=[tensorboard_callback])\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn():\n",
    "    def client_fn(cid: str):\n",
    "        i = int(cid)\n",
    "        return FLClient(cid, client_data[i][0], client_data[i][1], client_data[i][2], client_data[i][3]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        alpha = learning_rate\n",
    "        if server_round > 5:\n",
    "            alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"lr\": alpha,\n",
    "            \"local_epochs\": config.local_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "\n",
    "def get_evaluate_fn(x_test_sever, y_test_server):\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        # eval_model = model\n",
    "        eval_model = create_keras_model(input_shape=input_dim)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/multidigraph/server\".format(dtime) \n",
    "        # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "                                                  batch_size = cfg.config_fit.batch_size,\n",
    "                                                  callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "        y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "        if cfg.multi_class:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            scores = custom_acc_mc(y_test_server, y_pred)\n",
    "        else:\n",
    "            y_pred = np.transpose(y_pred)[0]\n",
    "            y_pred = list(\n",
    "                map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "            scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        results_final[\"centralities - MultiDiGraph\"][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "        results_final[\"centralities - MultiDiGraph\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "        if not cfg.multi_class:\n",
    "            for k in test_by_class.keys():\n",
    "                y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "                y_pred_class = np.transpose(y_pred_class)[0]\n",
    "                y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "                scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "                results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "                results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "        log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "        return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics):\n",
    "    print(f\"==>> weighted_average: {metrics}\")\n",
    "\n",
    "    total_examples = 0\n",
    "    federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "    for num_examples, m in metrics:\n",
    "        for k, v in m.items():\n",
    "            federated_metrics[k] += num_examples * v\n",
    "        total_examples += num_examples\n",
    "    return {k: v / total_examples for k, v in federated_metrics.items()}\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "    min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "    fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "    min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "    min_available_clients=len(client_data),  # total clients in the simulation\n",
    "    fit_metrics_aggregation_fn = weighted_average,\n",
    "    # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "    on_fit_config_fn=get_on_fit_config(\n",
    "        cfg.config_fit\n",
    "    ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "    evaluate_fn=get_evaluate_fn(test, test_labels),\n",
    ")  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from math import floor\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=generate_client_fn(),  # a function that spawns a particular client\n",
    "    # num_clients=cfg.n_clients,  # total number of clients\n",
    "    num_clients=len(client_data),  # total number of clients\n",
    "    config=fl.server.ServerConfig(\n",
    "        num_rounds=cfg.n_rounds\n",
    "        # num_rounds=5\n",
    "    ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "    strategy=strategy,  # our strategy of choice\n",
    "    client_resources={\n",
    "        \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "        \"num_gpus\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> history: {history}\")\n",
    "print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/multidigraph.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
