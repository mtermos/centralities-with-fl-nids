{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "from src.network.network_features import cal_betweenness_centrality\n",
    "from sklearn.model_selection import train_test_split\n",
    "# datasets is a list of available datasets descriptions containing: path, key columns names, and suitable complex network features\n",
    "from src.data.dataset_info import datasets\n",
    "\n",
    "\n",
    "\n",
    "with_sort_timestamp = False\n",
    "undersample_classes = True\n",
    "folder_path = \"folder path\"\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets[0]\n",
    "print(f\"==>> dataset1.name: {dataset1.name}\")\n",
    "df1 = pd.read_parquet(\"path to the dataset\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df1.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df1.drop_duplicates(subset=list(set(df1.columns) - set([dataset1.timestamp_col, dataset1.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset1.low_classes:\n",
    "    df1 = df1[~df1[dataset1.class_col].isin(dataset1.low_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = datasets[1]\n",
    "print(f\"==>> dataset2.name: {dataset2.name}\")\n",
    "df2 = pd.read_parquet(\"path to the dataset\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df2.drop_duplicates(subset=list(set(df2.columns) - set([dataset2.timestamp_col, dataset2.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset2.low_classes:\n",
    "    df2 = df2[~df2[dataset2.class_col].isin(dataset2.low_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + 'labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centralities(df):\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source=dataset1.src_ip_col,\n",
    "        target=dataset1.dst_ip_col,\n",
    "        create_using=nx.MultiDiGraph()\n",
    "    )\n",
    "\n",
    "    print(f\"===============\")\n",
    "    print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "    print(f\"===============\")\n",
    "\n",
    "    degrees = nx.degree_centrality(G)\n",
    "    betwe = cal_betweenness_centrality(G)\n",
    "    pagerank = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "    df[\"src_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_centralities_multidigraph(df):\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source=dataset1.src_ip_col,\n",
    "        target=dataset1.dst_ip_col,\n",
    "        create_using=nx.MultiDiGraph()\n",
    "    )\n",
    "\n",
    "    print(f\"===============\")\n",
    "    print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "    print(f\"===============\")\n",
    "\n",
    "    degrees = nx.degree_centrality(G)\n",
    "    betwe = cal_betweenness_centrality(G)\n",
    "    pagerank = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "    df[\"src_multidigraph_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_multidigraph_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_multidigraph_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_multidigraph_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_multidigraph_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_multidigraph_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test1, test2])\n",
    "test = add_centralities(test)\n",
    "test = add_centralities_multidigraph(test)\n",
    "test.to_parquet(folder_path + \"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "\n",
    "for cid, data_partition in enumerate(client_data):\n",
    "    \n",
    "    data_partition = add_centralities(data_partition)\n",
    "    data_partition = add_centralities_multidigraph(data_partition)\n",
    "\n",
    "    data_partition.to_parquet(folder_path + \"client_{}.parquet\".format(cid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
